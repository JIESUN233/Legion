/*

Copyright (c) 2018, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

namespace simt { namespace details { inline namespace v1 {
static inline __device__ void __simt_membar_block() { asm volatile("membar.cta;":::"memory"); }
static inline __device__ void __simt_fence_acq_rel_block() { asm volatile("fence.acq_rel.cta;":::"memory"); }
static inline __device__ void __simt_fence_sc_block() { asm volatile("fence.sc.cta;":::"memory"); }
static inline __device__ void __atomic_thread_fence_simt(int memorder, __thread_scope_block_tag) {
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_fence_acq_rel_block(); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_membar_block(); break;
#endif // __CUDA_ARCH__ >= 700
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_load_acquire_32_block(_A _ptr, _B& _dst) {asm volatile("ld.acquire.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_32_block(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_volatile_32_block(_A _ptr, _B& _dst) {asm volatile("ld.volatile.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_32_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_32_block(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_volatile_32_block(ptr, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELAXED: __simt_load_volatile_32_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B> static inline __device__ void __simt_load_acquire_64_block(_A _ptr, _B& _dst) {asm volatile("ld.acquire.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_64_block(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_volatile_64_block(_A _ptr, _B& _dst) {asm volatile("ld.volatile.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_64_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_64_block(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_volatile_64_block(ptr, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELAXED: __simt_load_volatile_64_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_32_block(_A _ptr, _B _src) { asm volatile("st.relaxed.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_32_block(_A _ptr, _B _src) { asm volatile("st.release.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_volatile_32_block(_A _ptr, _B _src) { asm volatile("st.volatile.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __simt_store_release_32_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_RELAXED: __simt_store_relaxed_32_block(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __simt_membar_block();
    case __ATOMIC_RELAXED: __simt_store_volatile_32_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_64_block(_A _ptr, _B _src) { asm volatile("st.relaxed.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_64_block(_A _ptr, _B _src) { asm volatile("st.release.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_volatile_64_block(_A _ptr, _B _src) { asm volatile("st.volatile.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __simt_store_release_64_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_RELAXED: __simt_store_relaxed_64_block(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __simt_membar_block();
    case __ATOMIC_RELAXED: __simt_store_volatile_64_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acq_rel_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acquire_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_relaxed_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_release_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_volatile_32_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_acquire_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_compare_exchange_acq_rel_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_compare_exchange_release_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_relaxed_32_block(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_volatile_32_block(ptr, old, old_tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_compare_exchange_volatile_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_volatile_32_block(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exchange_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exchange_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_exchange_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_add_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_and_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_and_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_and_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_max_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_max_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_max_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_max_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_min_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_min_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_min_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_min_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_or_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_or_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_or_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acquire_32_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_release_32_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_volatile_32_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_sub_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_sub_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_sub_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acq_rel_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acquire_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_relaxed_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_release_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_volatile_32_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_xor_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_xor_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_volatile_32_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_xor_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acq_rel_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acquire_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_relaxed_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_release_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_volatile_64_block(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_acquire_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_compare_exchange_acq_rel_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_compare_exchange_release_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_relaxed_64_block(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_volatile_64_block(ptr, old, old_tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_compare_exchange_volatile_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_volatile_64_block(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exchange_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exchange_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_exchange_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_and_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_and_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_and_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_max_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_max_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_max_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_max_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_min_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_min_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_min_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_min_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_or_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_or_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_or_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acquire_64_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_release_64_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_volatile_64_block(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_sub_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_sub_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_sub_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acq_rel_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acquire_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_relaxed_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_release_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_volatile_64_block(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_xor_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_xor_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_xor_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); __simt_membar_block(); break;
    case __ATOMIC_RELEASE: __simt_membar_block(); __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __simt_membar_device() { asm volatile("membar.gl;":::"memory"); }
static inline __device__ void __simt_fence_acq_rel_device() { asm volatile("fence.acq_rel.gpu;":::"memory"); }
static inline __device__ void __simt_fence_sc_device() { asm volatile("fence.sc.gpu;":::"memory"); }
static inline __device__ void __atomic_thread_fence_simt(int memorder, __thread_scope_device_tag) {
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_fence_acq_rel_device(); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_membar_device(); break;
#endif // __CUDA_ARCH__ >= 700
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_load_acquire_32_device(_A _ptr, _B& _dst) {asm volatile("ld.acquire.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_32_device(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_volatile_32_device(_A _ptr, _B& _dst) {asm volatile("ld.volatile.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_32_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_32_device(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_volatile_32_device(ptr, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELAXED: __simt_load_volatile_32_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B> static inline __device__ void __simt_load_acquire_64_device(_A _ptr, _B& _dst) {asm volatile("ld.acquire.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_64_device(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_volatile_64_device(_A _ptr, _B& _dst) {asm volatile("ld.volatile.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_64_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_64_device(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_volatile_64_device(ptr, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELAXED: __simt_load_volatile_64_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_32_device(_A _ptr, _B _src) { asm volatile("st.relaxed.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_32_device(_A _ptr, _B _src) { asm volatile("st.release.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_volatile_32_device(_A _ptr, _B _src) { asm volatile("st.volatile.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __simt_store_release_32_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_RELAXED: __simt_store_relaxed_32_device(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __simt_membar_device();
    case __ATOMIC_RELAXED: __simt_store_volatile_32_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_64_device(_A _ptr, _B _src) { asm volatile("st.relaxed.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_64_device(_A _ptr, _B _src) { asm volatile("st.release.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_volatile_64_device(_A _ptr, _B _src) { asm volatile("st.volatile.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __simt_store_release_64_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_RELAXED: __simt_store_relaxed_64_device(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __simt_membar_device();
    case __ATOMIC_RELAXED: __simt_store_volatile_64_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acq_rel_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acquire_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_relaxed_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_release_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_volatile_32_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_acquire_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_compare_exchange_acq_rel_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_compare_exchange_release_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_relaxed_32_device(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_volatile_32_device(ptr, old, old_tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_compare_exchange_volatile_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_volatile_32_device(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exchange_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exchange_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_exchange_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_add_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_and_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_and_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_and_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_max_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_max_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_max_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_max_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_min_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_min_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_min_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_min_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_or_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_or_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_or_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acquire_32_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_release_32_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_volatile_32_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_sub_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_sub_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_sub_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acq_rel_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acquire_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_relaxed_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_release_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_volatile_32_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_xor_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_xor_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_volatile_32_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_xor_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acq_rel_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acquire_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_relaxed_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_release_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_volatile_64_device(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_acquire_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_compare_exchange_acq_rel_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_compare_exchange_release_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_relaxed_64_device(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_volatile_64_device(ptr, old, old_tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_compare_exchange_volatile_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_volatile_64_device(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exchange_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exchange_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_exchange_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_and_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_and_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_and_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_max_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_max_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_max_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_max_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_min_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_min_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_min_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_min_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_or_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_or_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_or_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acquire_64_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_release_64_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_volatile_64_device(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_sub_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_sub_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_sub_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acq_rel_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acquire_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_relaxed_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_release_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_volatile_64_device(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_xor_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_xor_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_xor_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); __simt_membar_device(); break;
    case __ATOMIC_RELEASE: __simt_membar_device(); __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __simt_membar_system() { asm volatile("membar.sys;":::"memory"); }
static inline __device__ void __simt_fence_acq_rel_system() { asm volatile("fence.acq_rel.sys;":::"memory"); }
static inline __device__ void __simt_fence_sc_system() { asm volatile("fence.sc.sys;":::"memory"); }
static inline __device__ void __atomic_thread_fence_simt(int memorder, __thread_scope_system_tag) {
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_fence_acq_rel_system(); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __simt_membar_system(); break;
#endif // __CUDA_ARCH__ >= 700
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_load_acquire_32_system(_A _ptr, _B& _dst) {asm volatile("ld.acquire.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_32_system(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_volatile_32_system(_A _ptr, _B& _dst) {asm volatile("ld.volatile.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_32_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_32_system(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_volatile_32_system(ptr, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELAXED: __simt_load_volatile_32_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B> static inline __device__ void __simt_load_acquire_64_system(_A _ptr, _B& _dst) {asm volatile("ld.acquire.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_relaxed_64_system(_A _ptr, _B& _dst) {asm volatile("ld.relaxed.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_load_volatile_64_system(_A _ptr, _B& _dst) {asm volatile("ld.volatile.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_simt(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_acquire_64_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __simt_load_relaxed_64_system(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_load_volatile_64_system(ptr, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELAXED: __simt_load_volatile_64_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_32_system(_A _ptr, _B _src) { asm volatile("st.relaxed.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_32_system(_A _ptr, _B _src) { asm volatile("st.release.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_volatile_32_system(_A _ptr, _B _src) { asm volatile("st.volatile.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __simt_store_release_32_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_RELAXED: __simt_store_relaxed_32_system(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __simt_membar_system();
    case __ATOMIC_RELAXED: __simt_store_volatile_32_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _A, class _B> static inline __device__ void __simt_store_relaxed_64_system(_A _ptr, _B _src) { asm volatile("st.relaxed.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_release_64_system(_A _ptr, _B _src) { asm volatile("st.release.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _A, class _B> static inline __device__ void __simt_store_volatile_64_system(_A _ptr, _B _src) { asm volatile("st.volatile.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_simt(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __simt_store_release_64_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_RELAXED: __simt_store_relaxed_64_system(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __simt_membar_system();
    case __ATOMIC_RELAXED: __simt_store_volatile_64_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acq_rel_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acquire_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_relaxed_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_release_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_volatile_32_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_acquire_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_compare_exchange_acq_rel_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_compare_exchange_release_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_relaxed_32_system(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_volatile_32_system(ptr, old, old_tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_compare_exchange_volatile_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_volatile_32_system(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exchange_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exchange_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_exchange_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_add_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_and_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_and_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_and_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_max_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_max_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_max_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_max_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_min_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_min_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_min_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_min_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_or_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_or_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_or_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acquire_32_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_release_32_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_volatile_32_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_sub_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_sub_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_sub_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acq_rel_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acquire_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_relaxed_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_release_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_volatile_32_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_xor_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_xor_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_volatile_32_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_xor_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acq_rel_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acq_rel.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_acquire_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.acquire.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_relaxed_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.relaxed.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_release_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.release.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _A, class _B, class _C, class _D> static inline __device__ void __simt_compare_exchange_volatile_64_system(_A _ptr, _B& _dst, _C _cmp, _D _op) { asm volatile("atom.cas.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_simt(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_simt(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_acquire_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_compare_exchange_acq_rel_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_compare_exchange_release_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_relaxed_64_system(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_compare_exchange_volatile_64_system(ptr, old, old_tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_compare_exchange_volatile_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_compare_exchange_volatile_64_system(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_exchange_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.exch.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_simt(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_exchange_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_exchange_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_exchange_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_exchange_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_exchange_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_add_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.add.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_and_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.and.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_and_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_and_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_and_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_and_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_and_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_max_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.max.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_max_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_max_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_max_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_max_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_max_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_max_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_min_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.min.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_min_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_min_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_min_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_min_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_min_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_min_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_or_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.or.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_or_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_or_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_or_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_or_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_or_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_acquire_64_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_release_64_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_sub_volatile_64_system(_A _ptr, _B& _dst, _C _op) { _op = -_op;
asm volatile("atom.add.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_sub_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_sub_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_sub_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_sub_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_sub_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acq_rel_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_acquire_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_relaxed_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_release_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _A, class _B, class _C> static inline __device__ void __simt_fetch_xor_volatile_64_system(_A _ptr, _B& _dst, _C _op) { asm volatile("atom.xor.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename simt::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_simt(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_xor_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_xor_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_xor_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_xor_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_xor_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_simt(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __simt_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __simt_fetch_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __simt_fetch_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __simt_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); __simt_membar_system(); break;
    case __ATOMIC_RELEASE: __simt_membar_system(); __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __simt_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
} } }
